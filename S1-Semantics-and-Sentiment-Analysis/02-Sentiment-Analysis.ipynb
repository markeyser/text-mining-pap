{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Let's start to investigate sentiment analysis. The goal is to find commonalities between documents, with the understanding that similarly *combined* vectors should correspond to similar sentiments.\n",
    "\n",
    "While the scope of sentiment analysis is very broad, we will focus our work in two ways.\n",
    "\n",
    "### 1. Polarity classification\n",
    "We won't try to determine if a sentence is objective or subjective, fact or opinion. Rather, we care only if the text expresses a *positive*, *negative* or *neutral* opinion.\n",
    "### 2. Document level scope\n",
    "We'll also try to aggregate all of the sentences in a document or paragraph, to arrive at an overall opinion.\n",
    "### 3. Coarse analysis\n",
    "We won't try to perform a fine-grained analysis that would determine the degree of positivity/negativity. That is, we're not trying to guess how many stars a reviewer awarded, just whether the review was positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broad Steps:\n",
    "* First, consider the text being analyzed. A model trained on paragraph-long movie reviews might not be effective on tweets. Make sure to use an appropriate model for the task at hand.\n",
    "* Next, decide the type of analysis to perform. In the next session on text classification we will use a bag-of-words technique that considered only single tokens, or *unigrams*. Some rudimentary sentiment analysis models go one step further, and consider two-word combinations, or *bigrams*. In this section, we'd like to work with complete sentences, and for this we're going to import a trained **NLTK lexicon** called ***VADER***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK's VADER module\n",
    "\n",
    "Take a look to the NLTK home page [here](https://www.nltk.org/). To learn more about [VADER](https://pdfs.semanticscholar.org/a6e4/a2532510369b8f55c68f049ff11a892fefeb.pdf)\n",
    "\n",
    "VADER is an NLTK module that provides sentiment scores based on words used (\"completely\" boosts a score, while \"slightly\" reduces it), on capitalization & punctuation (\"GREAT!!!\" is stronger than \"great.\"), and negations (words like \"isn't\" and \"doesn't\" affect the outcome).\n",
    "<br>To view the source code visit https://www.nltk.org/_modules/nltk/sentiment/vader.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the VADER lexicon.** You only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">NOTE: At the time of this writing there's a <a href='https://github.com/nltk/nltk/issues/2053'>known issue</a> with SentimentIntensityAnalyzer that raises a harmless warning on loading<br>\n",
    "<tt><font color=black>&emsp;UserWarning: The twython library has not been installed.<br>&emsp;Some functionality from the twitter package will not be available.</tt>\n",
    "\n",
    "This is due to be fixed in an upcoming NLTK release. For now, if you want to avoid it you can (optionally) install the NLTK twitter library with<br>\n",
    "<tt><font color=black>&emsp;conda install nltk[twitter]</tt><br>or<br>\n",
    "<tt><font color=black>&emsp;pip3 install -U nltk[twitter]</tt></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER's `SentimentIntensityAnalyzer()` takes in a string and returns a dictionary of scores in each of four categories:\n",
    "* negative\n",
    "* neutral\n",
    "* positive\n",
    "* compound *(computed by normalizing the scores above)*\n",
    "\n",
    "\n",
    "So let's create a really simple string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'This was a good movie.'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you get back this dictionary which has some negative value, a neutral value, a positive value and then a compound value which essentially normalizing these three values here.\n",
    "\n",
    "So, as we expect there is no negative value since this is a good movie. It has some neutral words or tones in it and then it has also some positive tones. And the max value for any of these four scorews is 1.0.\n",
    "\n",
    "So now let's try a more complicated string. Notice we're going to capitalize \"ever made\" and have three exclamation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'This was the best, most awesome movie EVER MADE!!!'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously mentioned, VADER is smart enough to understand things like repeated punctuation and capitalization.\n",
    "\n",
    "And here we can see it's again more positive than the previous one.\n",
    "\n",
    "And we can see here that the compound score is much more positive because neutral also dropped.\n",
    "\n",
    "Finally let's go ahead and have a very negative string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'This was the worst film to ever disgrace the screen.'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quite a negative review. Let's see if the VADER picks it up.\n",
    "\n",
    "And here we can see that now there is no positive, it's just neutral and negative and so happens is the compound score then becomes negative. \n",
    "\n",
    "- So we can see here a compound score of zero would be completely neutral\n",
    "- A compound score above zero indicates some sort of positive score \n",
    "- A compound score below zero indicates some sort of negative score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use VADER to analyze Amazon Reviews\n",
    "For this exercise we're going to apply `SentimentIntensityAnalyzer` to a dataset of 10,000 Amazon reviews. Like our movie reviews datasets, these are labeled as either \"pos\" or \"neg\". At the end we'll determine the accuracy of our sentiment analysis with VADER.\n",
    "\n",
    "> The text is tab separated. So you need to also indicate that the separator is backslash `t` for tab separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you read that in you should be able to view it by simply calling the `head()` of that data frame. And essentially what we have here are:\n",
    "\n",
    "- `labels`: `pos` for positive or `neg` for a negative \n",
    "- `review`: the actual text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to get an idea of how many positive or negative labels we have we can say `df` pass passing the `label` column and then simply call `value_counts()` and we can see here we have slightly more negative reviews than positive reviews but, overall it looks like we have around 10,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data:\n",
    "\n",
    "So what we're gonna do now is do a little bit of cleaning of the data just to double check that we have no empty records and then we're going to run a first review through VADER.\n",
    "\n",
    "Recall that our moviereviews.tsv file contained empty records. Let's check to see if any exist in amazonreviews.tsv.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "This is going to drop anything that's missing, we're going to do is drop anything that has a empty whitespace value.\n",
    "\n",
    "```python\n",
    "df.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "Now for your data sets depending where you get them you may or may not have this but, it's always a good idea.\n",
    "\n",
    "So I'm just saying for index for label and for review:\n",
    "\n",
    "```python\n",
    "for i, lb, rv in df.itertuples()\n",
    "```\n",
    "\n",
    "So those are kind of place holders there. Now let's use `df.itertuples()` so, here everything is just going to be returned as a tuple where I have the index the label and then the review text.\n",
    "\n",
    "So for i, label and review:\n",
    "\n",
    "```python\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "```\n",
    "\n",
    "I'm going to say if the type of the review is equal to the string type, then I'm going to check that the review is space, essentially checking whether or not it's a space there. And if it's true, I'm going to take a list of blanks `blanks = []` and  simply say `blanks.append()` and then we'll plan that index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE NaN VALUES AND EMPTY STRINGS:\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "\n",
    "df.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we run this let's go ahead and check on blanks. See if we had any blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we did it. This list is empty. So we don't need to drop anything. But if we did have some index positions that were blanks, we simply need to say:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(blanks,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again since we don't have any we don't actually need to run that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there were no empty records. Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the first review through VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're going to do is continue on and run a first review through VADER. We're going to just run the first review on it.\n",
    "\n",
    "Let's checkout the text of the first review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here it's quite positive, \"the soundtrack was beautiful\",  \"game music\", exclamation points \"best music\" etc.\n",
    "\n",
    "So check now the polarity score here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(df.loc[0]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it has a very small amount of negativity that Vader picked up. It could be small phrases that get confusing for Vader things like \"anyone who cares to listen!\" may be kind of negative in a slight sense but it's actually a very small negativity. In fact most of it is neutral or slightly positive which means a compound score is extremely positive, which if we take a look at that first label was positive. So looks like Vader is actually able to select that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our first review was labeled \"positive\", and earned a positive compound score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Scores and Labels to the DataFrame\n",
    "\n",
    "So now let's go ahead and add scores and labels to the data frame.\n",
    "\n",
    "In this next section we'll add columns to the original DataFrame to store polarity_score dictionaries, extracted compound scores, and new \"pos/neg\" labels derived from the compound score. We'll use this last column to perform an accuracy test.\n",
    "\n",
    "So we're going to create a new column named `scores` that is equal to `review` and then we're going to call in `apply()` method in order to essentially apply `sid.polarity_scores()` to every single review in our data frame. So we'll say lambda take that review and then apply `sid.polarity_scores()` to that particular review:\n",
    "\n",
    "> More about how to use lambda funcitons [here](https://realpython.com/python-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we run that and this may take a little bit of time because it is running this whole polarity of course function on every single review. But once you have that you can go ahead and check out the ahead of the data frame and then you'll get back in your column `scores` that contains this dictionary.\n",
    "\n",
    "But we really just want to be of the compound score. So let's go ahead and create a new `compound` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice these first five labels (`label`) they're all positive and it looks like the compound score (`compund`) is also all positive.\n",
    "\n",
    "So let's go ahead and based off this compound score do a little bit of logic and say:\n",
    "\n",
    "- if it's greater than zero then it's positive.\n",
    "\n",
    "- If it's less than zero it's negative \n",
    "\n",
    "And then we'll compare these compound scores to the true labels that we already know. \n",
    "\n",
    "So we're going to say one last column of our creation `comp_score`. \n",
    "\n",
    "Essentially changing the score into a string that matches our current label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looks like we're matching up on the first five.\n",
    "\n",
    "But let's go ahead and have an overall report on the accuracy comparing the Vader compound score labels to the manual labels from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on Accuracy\n",
    "Finally, we'll use scikit-learn to determine how close VADER came to our original 10,000 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first just get the accuracy score and we can do that by simply saying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['label'],df['comp_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So essentially we're comparing how well that Vader perform against what was manually labeled. So the `label` column was manual label, essentially a person read these reviews and decided whether or not they're positive or negative.\n",
    "\n",
    "So if we run their accuracy score we get an accuracy of 0.71. If we were to randomly choose positives and negatives we'd be probably getting an accuracy score of around 0.5.\n",
    "\n",
    "So we can see we're doing better than random guessing which is quite good given the fact that we're essentially just running one line of code to get the polarity scores. So it's definitely not bad considering how simple it is to run this process.\n",
    "\n",
    "\n",
    "Let's go ahead and print the classification report we'll say print classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['label'],df['comp_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll pass in the true `label` that we know and then our calculated `comp_score` so we'll run this and then we can see our precision, recall, and F1 score and we can also compare negative versus positive.\n",
    "\n",
    "So it looks like the Vader has a little bit of trouble with negative reviews versus positive reviews. And if you take a look at some of these Amazon reviews some of these strings and some of the text is sometimes a bit hard to read and sometimes it's also sarcastic which means it's really hard to detect. So sarcasm is almost impossible to detect for something like Vader.\n",
    "\n",
    "And then finally let's print out a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(df['label'],df['comp_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This tells us that VADER correctly identified an Amazon review as \"positive\" or \"negative\" roughly 71% of the time. It is not performing well with negative reviews. This performs not bad considering how simple the process is but, it is also not excellent compared to maybe some state-of-the-art deep learning methods for sentiment analysis.\n",
    "\n",
    "\n",
    "\n",
    "## Up Next: Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
