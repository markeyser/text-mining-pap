{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Develop an Embedding + CNN Model for Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can find the demo here:\n",
    "\n",
    "## https://github.com/markeyser/sentiment-analysis-CNN-demo\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this demo, you will discover how to develop **word embedding models** with **convolutional neural networks** (**CNN**) to classify movie reviews. After completing this demo, you will know:\n",
    "\n",
    "- How to prepare movie review text data for classification with deep learning methods.\n",
    "- How to develop a neural classification model with word embedding and convolutional layers.\n",
    "- How to evaluate the developed a neural classification model.\n",
    "\n",
    "Let's get started by reviewing some key concepts:\n",
    "\n",
    "- **Sentiment Analysis:**\n",
    "\n",
    "<center><img src=\"../imgs/sentiment_class_slide.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "\n",
    "- **Model Estimation Process:**\n",
    "\n",
    "<center><img src=\"../imgs/modeling-process.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "\n",
    "- **Word embeddings** are a technique for representing text where <u>different words with similar meaning</u> have a similar real-valued vector representation. \n",
    "\n",
    "<center><img src=\"../imgs/words-embedding-01.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "\n",
    "They are a key breakthrough that has\n",
    "led to great performance of neural network models on a suite of challenging natural language\n",
    "processing problems. \n",
    "\n",
    "<center><img src=\"../imgs/words-embeddings-02.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/words-embeddings-03.png\" alt=\"Drawing\" width = \"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This demo is divided into the following parts:\n",
    "1. Movie Review Dataset\n",
    "2. Data Preparation  \n",
    "3. Train CNN With Embedding Layer\n",
    "4. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Dataset\n",
    "\n",
    "- The **Movie Review Data** is a collection of movie reviews retrieved from the [imdb.com](https://www.imdb.com/) website in\n",
    "the early 2000s by Bo Pang and Lillian Lee. \n",
    "- The reviews were collected and made available as part of their research on natural language processing. \n",
    "- The reviews were originally released in 2002, but an updated and cleaned up version was released in 2004, referred to as *[v2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.txt)*. \n",
    "- The dataset is comprised of **1,000 positive** and **1,000 negative** movie reviews drawn from an archive of the `rec.arts.movies.reviews` newsgroup hosted at [IMDd](https://www.imdb.com/). The authors refer to this corpus as the ***polarity dataset***.\n",
    "\n",
    "The data has been cleaned up somewhat, for example:\n",
    "- The dataset is comprised of only English reviews.\n",
    "- All text has been converted to lowercase.\n",
    "- There is white space around punctuation like periods, commas, and brackets.\n",
    "- Text has been split into one sentence per line.\n",
    "\n",
    "The data has been used for a few related natural language processing tasks. For classification,\n",
    "the performance of classical models (such as Support Vector Machines) on the data is in the\n",
    "range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see\n",
    "results as high as 86% with 10-fold cross-validation. This gives us a ballpark of low-to-mid 80s\n",
    "if we were looking to use this dataset in experiments on modern methods.\n",
    ">... depending on choice of downstream polarity classifier, we can achieve highly\n",
    "statistically significant improvement (from 82.8% to 86.4%)\n",
    ">\n",
    ">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, 2004.\n",
    "\n",
    "You can download the dataset from here:\n",
    "\n",
    "Movie Review Polarity Dataset (review polarity.tar.gz, 3MB).\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "\n",
    "After unzipping the file, you will have a directory called `txt_sentoken` with two sub-directories containing the text *neg* and *pos* for negative and positive reviews. Reviews are stored one per file with a naming convention `cv000` to `cv999` for each of *neg* and *pos*. Next, let's look at loading the text data.\n",
    "\n",
    "### Rating decision:\n",
    "\n",
    "<center><img src=\"../imgs/rating.png\" alt=\"Drawing\" width = \"600\"/></center>\n",
    "\n",
    "### Further information:\n",
    "\n",
    "- [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\n",
    "- [A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, 2004.](https://xxx.lanl.gov/abs/cs/0409058)\n",
    "- Dataset Readme [v2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.txt) and [v1.1](http://www.cs.cornell.edu/people/pabo/movie-review-data/README.1.1).\n",
    "- [IMDb](https://en.wikipedia.org/wiki/IMDb) on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this section, we will look at 3 things:\n",
    "\n",
    "1. Separation of data into **training** and **test** sets.\n",
    "2. Loading and cleaning the data to **remove punctuation** and **numbers**.\n",
    "3. Defining a **vocabulary** of preferred words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train and Test Sets\n",
    "\n",
    "We are developing a system that can predict the sentiment of a textual movie review as either positive or negative. \n",
    "- This means that after the model is developed, we will need to make predictions on new textual reviews - scoring. \n",
    "- This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the\n",
    "model. \n",
    "\n",
    "We will ensure that this constraint is built into the **evaluation** of our models by **splitting the training and test datasets prior to any data preparation**. This means that any knowledge in the data in the test set that could help us better prepare the data (e.g. the words used) are unavailable in the preparation of data used for training the model.\n",
    "\n",
    "That being said, we will use the last **100 positive reviews** and the last **100 negative reviews**\n",
    "as a **test** set (**200 reviews**) and the remaining **1,800 reviews** as the **training** dataset. \n",
    "\n",
    "| Reviews           | Original Set  | Training Set  | Test Set  |\n",
    "| -------------:    | -------------:| -------------:| ---------:|\n",
    "| Positive Reviews  | 1,000         |           900 |      100  |\n",
    "| Negative Reviews  | 1,000         |           900 |      100  |\n",
    "| Total Reviews     | 2,000         |         1,800 |      200  |\n",
    "\n",
    "This is a\n",
    "**90% train, 10% split of the data**. The split can be imposed easily by using the filenames of the\n",
    "reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards\n",
    "are for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Reviews\n",
    "\n",
    "In this section, we will look at loading individual text files, then processing the directories of\n",
    "files. \n",
    "\n",
    "- We will assume that the review data is downloaded and available in the current working\n",
    "directory in the folder `txt_sentoken`. \n",
    "- We can load an individual text file by **opening** it, **reading** in the ASCII text, and **closing** the file. \n",
    "\n",
    "> The text is encoding as ASCII, the most basic encoding. To know more: [A simple explanation of character encoding in python](http://www.cogsci.nl/blog/a-simple-explanation-of-character-encoding-in-python.html)\n",
    "\n",
    "This is standard file handling stuff. For example, we can load the first negative review file `cv000_29416.txt` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load one file\n",
    "filename = '../data/txt_sentoken/neg/cv000_29416.txt'  \n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "# close the file\n",
    "file.close()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines. We can\n",
    "turn this into a function called `load_doc()` that takes a filename of the document to load and\n",
    "returns the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read onl\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "    # print the content of the file\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn\\'t snag this one correctly . \\nthey seem to have taken this pretty neat concept , but executed it terribly . \\nso what are the problems with the movie ? \\nwell , its main problem is that it\\'s simply too jumbled . \\nit starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what\\'s going on . \\nthere are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \\nnow i personally don\\'t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film\\'s biggest problem . \\nit\\'s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \\nand do they make things entertaining , thrilling or even engaging , in the meantime ? \\nnot really . \\nthe sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn\\'t the make the film all that more entertaining . \\ni guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \\ni mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \\nokay , we get it . . . there \\nare people chasing her and we don\\'t know who they are . \\ndo we really need to see it over and over again ? \\nhow about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \\napparently , the studio took this film away from its director and chopped it up themselves , and it shows . \\nthere might\\'ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \\nthe actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \\nbut my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character\\'s unraveling . \\noverall , the film doesn\\'t stick because it doesn\\'t entertain , it\\'s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \\noh , and by the way , this is not a horror or teen slasher flick . . . it\\'s \\njust packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \\nit also wrapped production two years ago and has been sitting on the shelves ever since . \\nwhatever . . . skip \\nit ! \\nwhere\\'s joblo coming from ? \\na nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the file to load\n",
    "filename = '../data/txt_sentoken/neg/cv000_29416.txt' \n",
    "load_doc(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two directories each with 1,000 documents each. We can process each directory in\n",
    "turn by first getting a list of files in the directory using the `listdir()` function, then loading\n",
    "each file in turn. For example, we can load each document in the negative directory using the\n",
    "`load_doc()` function to do the actual loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# specify directory to load\n",
    "directory = '../data/txt_sentoken/neg'\n",
    "# walk through all files in the folder\n",
    "for filename in listdir(directory):\n",
    "    # skip files that do not have the right extension\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        next\n",
    "    # create the full path of the file to open\n",
    "    path = directory + '/' + filename\n",
    "    # load document\n",
    "    doc = load_doc(path)\n",
    "    print('Loaded %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn the processing of the documents into a function as well and use it as a template\n",
    "later for developing a function to clean all documents in a folder. For example, below we define\n",
    "a `process_docs()` function to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "\n",
    "# specify directory to load\n",
    "directory = '../data/txt_sentoken/neg'\n",
    "process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to load the movie review text data, let's look at cleaning it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Reviews\n",
    "\n",
    "In this section, we will look at what data cleaning we might want to do to the **movie review\n",
    "data**. We will assume that we will be using a bag-of-words model or perhaps a word embedding\n",
    "that does not require too much preparation.\n",
    "\n",
    "#### Split into Tokens\n",
    "\n",
    "First, let's load one document and look at the raw tokens split by white space. \n",
    "\n",
    "- We will use the `load_doc()` function developed in the previous section. \n",
    "- We can use the `split()` function to split the loaded document into tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      "\n",
      "salaries of hollywood top actors are getting obscenely large these days and many find this to be the main reason for skyrocketing movie budgets . \n",
      "actors who demand such salaries might be greedy , but in some instances they are quite justified , because many films would never be watched or even made without their participation . \n",
      "proof for that can be found even in the realm of low-budget movies , and one fine example is breakaway , 1995 thriller directed by sean dash and starring ( in ) famous figure skater tonya harding . \n",
      "face of tonya harding is most prominently featured on movie's poster , but the main star of the film is terri thompson who plays myra , attractive woman who works as a courier for gangster . \n",
      "one day she decides to retire , but her employers are anything but enthusiastic about that . \n",
      "realising that her life suddenly became worthless , myra starts running for her life , followed by professional assassins . \n",
      "terri thompson being the actual star of the film instead of tonya harding becomes quite understandable after the scenes that feature former figure skater . \n",
      "although tonya harding displays convincing martial arts abilities , her acting leaves much to be desired . \n",
      "on the other hand , her disappointing efforts are hardly out of place in the film that lacks originality , believable characters and situations and actually represents anything that gave b-films a bad name . \n",
      "martin sheen's brother joe estevez , whose character looks like he had entered from another movies' set , is the only bright spot of breakaway . \n",
      "unfortunately , he appears in this film too little too late to prevent viewers from realising why tonya harding's silver screen debut proved to be her last film . \n",
      "\n",
      "Tokenized Review:\n",
      "\n",
      "['salaries', 'of', 'hollywood', 'top', 'actors', 'are', 'getting', 'obscenely', 'large', 'these', 'days', 'and', 'many', 'find', 'this', 'to', 'be', 'the', 'main', 'reason', 'for', 'skyrocketing', 'movie', 'budgets', '.', 'actors', 'who', 'demand', 'such', 'salaries', 'might', 'be', 'greedy', ',', 'but', 'in', 'some', 'instances', 'they', 'are', 'quite', 'justified', ',', 'because', 'many', 'films', 'would', 'never', 'be', 'watched', 'or', 'even', 'made', 'without', 'their', 'participation', '.', 'proof', 'for', 'that', 'can', 'be', 'found', 'even', 'in', 'the', 'realm', 'of', 'low-budget', 'movies', ',', 'and', 'one', 'fine', 'example', 'is', 'breakaway', ',', '1995', 'thriller', 'directed', 'by', 'sean', 'dash', 'and', 'starring', '(', 'in', ')', 'famous', 'figure', 'skater', 'tonya', 'harding', '.', 'face', 'of', 'tonya', 'harding', 'is', 'most', 'prominently', 'featured', 'on', \"movie's\", 'poster', ',', 'but', 'the', 'main', 'star', 'of', 'the', 'film', 'is', 'terri', 'thompson', 'who', 'plays', 'myra', ',', 'attractive', 'woman', 'who', 'works', 'as', 'a', 'courier', 'for', 'gangster', '.', 'one', 'day', 'she', 'decides', 'to', 'retire', ',', 'but', 'her', 'employers', 'are', 'anything', 'but', 'enthusiastic', 'about', 'that', '.', 'realising', 'that', 'her', 'life', 'suddenly', 'became', 'worthless', ',', 'myra', 'starts', 'running', 'for', 'her', 'life', ',', 'followed', 'by', 'professional', 'assassins', '.', 'terri', 'thompson', 'being', 'the', 'actual', 'star', 'of', 'the', 'film', 'instead', 'of', 'tonya', 'harding', 'becomes', 'quite', 'understandable', 'after', 'the', 'scenes', 'that', 'feature', 'former', 'figure', 'skater', '.', 'although', 'tonya', 'harding', 'displays', 'convincing', 'martial', 'arts', 'abilities', ',', 'her', 'acting', 'leaves', 'much', 'to', 'be', 'desired', '.', 'on', 'the', 'other', 'hand', ',', 'her', 'disappointing', 'efforts', 'are', 'hardly', 'out', 'of', 'place', 'in', 'the', 'film', 'that', 'lacks', 'originality', ',', 'believable', 'characters', 'and', 'situations', 'and', 'actually', 'represents', 'anything', 'that', 'gave', 'b-films', 'a', 'bad', 'name', '.', 'martin', \"sheen's\", 'brother', 'joe', 'estevez', ',', 'whose', 'character', 'looks', 'like', 'he', 'had', 'entered', 'from', 'another', \"movies'\", 'set', ',', 'is', 'the', 'only', 'bright', 'spot', 'of', 'breakaway', '.', 'unfortunately', ',', 'he', 'appears', 'in', 'this', 'film', 'too', 'little', 'too', 'late', 'to', 'prevent', 'viewers', 'from', 'realising', 'why', 'tonya', \"harding's\", 'silver', 'screen', 'debut', 'proved', 'to', 'be', 'her', 'last', 'film', '.']\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load the document\n",
    "filename = '../data/txt_sentoken/neg/cv993_29565.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(\"Original Review:\\n\")\n",
    "print(text)\n",
    "print(\"Tokenized Review:\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation, stop words, ...\n",
    "\n",
    "The text data is already pretty clean; not much preparation is required.\n",
    "\n",
    "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
    "\n",
    "- Remove **punctuation from words** (e.g. 'what's').\n",
    "- Removing tokens that are **just punctuation** (e.g. '-').\n",
    "- Removing tokens that contain **numbers** (e.g. '10/10').\n",
    "- Remove tokens that have **one character** (e.g. 'a').\n",
    "- Remove tokens that **don't have much meaning** (e.g. 'and').\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- We can filter out punctuation from tokens using **regular expressions**.\n",
    "- We can remove tokens that are just punctuation or contain numbers by using an `isalpah()` check on each token.\n",
    "- We can remove **English stop words** using the list loaded using **NLTK**.\n",
    "- We can filter out short tokens by checking their length and remove all words that have a length â‰¤ 1 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the example gives a much cleaner looking list of tokens:\n",
      "\n",
      "['salaries', 'hollywood', 'top', 'actors', 'getting', 'obscenely', 'large', 'days', 'many', 'find', 'main', 'reason', 'skyrocketing', 'movie', 'budgets', 'actors', 'demand', 'salaries', 'might', 'greedy', 'instances', 'quite', 'justified', 'many', 'films', 'would', 'never', 'watched', 'even', 'made', 'without', 'participation', 'proof', 'found', 'even', 'realm', 'lowbudget', 'movies', 'one', 'fine', 'example', 'breakaway', 'thriller', 'directed', 'sean', 'dash', 'starring', 'famous', 'figure', 'skater', 'tonya', 'harding', 'face', 'tonya', 'harding', 'prominently', 'featured', 'movies', 'poster', 'main', 'star', 'film', 'terri', 'thompson', 'plays', 'myra', 'attractive', 'woman', 'works', 'courier', 'gangster', 'one', 'day', 'decides', 'retire', 'employers', 'anything', 'enthusiastic', 'realising', 'life', 'suddenly', 'became', 'worthless', 'myra', 'starts', 'running', 'life', 'followed', 'professional', 'assassins', 'terri', 'thompson', 'actual', 'star', 'film', 'instead', 'tonya', 'harding', 'becomes', 'quite', 'understandable', 'scenes', 'feature', 'former', 'figure', 'skater', 'although', 'tonya', 'harding', 'displays', 'convincing', 'martial', 'arts', 'abilities', 'acting', 'leaves', 'much', 'desired', 'hand', 'disappointing', 'efforts', 'hardly', 'place', 'film', 'lacks', 'originality', 'believable', 'characters', 'situations', 'actually', 'represents', 'anything', 'gave', 'bfilms', 'bad', 'name', 'martin', 'sheens', 'brother', 'joe', 'estevez', 'whose', 'character', 'looks', 'like', 'entered', 'another', 'movies', 'set', 'bright', 'spot', 'breakaway', 'unfortunately', 'appears', 'film', 'little', 'late', 'prevent', 'viewers', 'realising', 'tonya', 'hardings', 'silver', 'screen', 'debut', 'proved', 'last', 'film']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load the document\n",
    "filename = '../data/txt_sentoken/neg/cv993_29565.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "tokens = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "# filter out short tokens\n",
    "tokens = [word for word in tokens if len(word) > 1]\n",
    "print(\"Running the example gives a much cleaner looking list of tokens:\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'through', 'it', 'before', 'once', \"don't\", 'yours', 'wasn', 'because', 'doesn', 'a', 'has', 'ours', 'we', 'yourself', 're', 'what', \"she's\", 'just', 'i', 'which', 'hasn', 'down', 'its', 'him', 'between', 'will', \"haven't\", 'd', 'as', 'you', 'y', 'too', 'myself', 'at', 'about', 'with', 'aren', 'itself', 'for', 'won', 'few', 'had', 'shouldn', 'doing', 'is', 'over', 'if', \"needn't\", 'the', 'were', 'does', 'be', 'out', \"weren't\", 'me', 'above', 'they', \"wasn't\", 'so', 'after', 'who', 'and', 'an', 'don', \"couldn't\", 'not', 'by', 'all', 'any', \"you'll\", 'did', 'he', \"that'll\", \"shan't\", 'below', 'she', 'was', 'from', \"hasn't\", 'weren', 'having', 'each', 'are', 'needn', 'whom', 't', 'll', \"doesn't\", 'own', 've', 'have', 'up', 'both', \"shouldn't\", 'into', 'there', 'my', 'those', 'these', 'am', \"it's\", 'further', 'why', 'shan', 'such', 'hers', 'this', 'should', 'some', 'been', \"you're\", 'our', \"aren't\", 'his', 'can', 'than', 'of', 'ourselves', 'theirs', 'other', 'to', 'haven', 'or', 'more', 'when', 'her', 'hadn', 'in', \"mightn't\", 'on', 'under', 'your', 'only', 'isn', 'nor', 'again', 'then', 'mightn', 'but', 'ma', 'couldn', 'most', 'how', 'no', \"isn't\", 'here', 'during', 'while', \"hadn't\", 'herself', 'until', \"you've\", 'them', 'being', 'yourselves', \"won't\", \"wouldn't\", 'that', 'o', 'ain', 'where', 'against', 'm', \"mustn't\", \"should've\", \"didn't\", 'now', 'mustn', 'wouldn', 'do', 'their', 'very', 'same', 'off', 's', 'themselves', 'himself', 'didn', \"you'd\"}\n"
     ]
    }
   ],
   "source": [
    "# English stop words in NLTK\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this into a function called `clean_doc()` and test it on another review, this time\n",
    "a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Again, the cleaning procedure seems to produce a good set of tokens, at least as a first cut:\n",
      "\n",
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load the document\n",
    "filename = '../data/txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(\"Again, the cleaning procedure seems to produce a good set of tokens, at least as a first cut:\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more cleaning steps we could take and I leave them to your imagination.\n",
    "Next, let's look at how we can manage a preferred vocabulary of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Vocabulary\n",
    "\n",
    "It is important to define a vocabulary of known words when using a text model. The more words, the larger the representation of documents, therefore it is important to constrain the\n",
    "words to only those believed to be predictive. This is diffcult to know beforehand and often it is important to test different hypotheses about how to construct a useful vocabulary. \n",
    "\n",
    "We can develop a vocabulary as a `Counter`, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the\n",
    "counter (a new function called `add_doc_to_vocab()`) and we can step over all of the reviews in\n",
    "the negative directory and then the positive directory (a new function called `process_docs()`). The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our template above for processing all documents in a directory called\n",
    "`process_docs()` and update it to `call_add_doc_to_vocab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all of this together and develop a full vocabulary from all documents in the\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('../data/txt_sentoken/pos', vocab)\n",
    "process_docs('../data/txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a vocabulary with all documents in the dataset, including\n",
    "positive and negative reviews. We can see that there are a little over 44,276 unique words across\n",
    "all reviews and the top 3 words are film, one, and movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example shows that we have a vocabulary of **44,276** words. We also can see a sample of the top 50 most used words in the movie reviews. \n",
    "\n",
    ">Note that this vocabulary was constructed based on only those reviews in the training dataset.\n",
    "\n",
    "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews. For example, the following snippet will retrieve only the tokens that appear **2 or more times** in all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above example with this addition shows that the vocabulary size drops by a little more than half its size, from **44,276** to **25,767** words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the vocabulary can be saved to a new file called `vocab.txt` that we can later load\n",
    "and use to filter movie reviews prior to encoding them for modeling. We define a new function called `save_list()` that saves the vocabulary to file, with one word per line. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, '../output/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the min occurrence filter on the vocabulary and saving it to file, you should now\n",
    "have a new file called `vocab.txt` with only the words we are interested in. The order of words in your file will differ, but should look something like the following:\n",
    "\n",
    "```\n",
    "aberdeen\n",
    "dupe\n",
    "burt\n",
    "libido\n",
    "hamlet\n",
    "arlene\n",
    "available\n",
    "corners\n",
    "web\n",
    "columbia\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to look at extracting features from the reviews ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN with Embedding Layer\n",
    "\n",
    "In this section, we will look at 4 things:\n",
    "\n",
    "1. Load the vocabulary and filter out tokens not in the vocabulary\n",
    "2. Combine the reviews into a single train or test set and define the class labels\n",
    "3. Enconde each document as a sequence of integers\n",
    "4. Define the neural network model\n",
    "\n",
    "In this section, we will learn a **word embedding while training a convolutional neural network** on the classification problem. \n",
    "\n",
    ">A **word embedding** is a way of representing text where each word in\n",
    "the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors\n",
    "are learned in such a way that words that have similar meanings will have similar representation\n",
    "in the vector space (close in the vector space). This is a more expressive representation for text\n",
    "than more classical methods like bag-of-words, where relationships between words or tokens are\n",
    "ignored, or forced in bigram and trigram approaches.\n",
    "\n",
    "<center><img src=\"../imgs/words-embedding-01.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/words-embeddings-02.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/words-embeddings-03.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/word-emb-0.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/word-emb-1.png\" alt=\"Drawing\" width = \"500\"/></center>\n",
    "\n",
    "More information [here](https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer)\n",
    "\n",
    "Accuracy using the Movie Review Dataset:\n",
    "\n",
    "| Technique                   | Accuracy (Test) \n",
    "| -------------:              | -------------:|\n",
    "| Train embedding layer       | 88%         |   \n",
    "| Train word2vec embedding    | 57.5%         |   \n",
    "| Use a pre-trained embedding | 76%         |   \n",
    "\n",
    "\n",
    "The real valued vector representation for words can be learned while training the neural\n",
    "network. We can do this in the **Keras** deep learning library using the **`Embedding` layer**. \n",
    "\n",
    "### Load the vocabulary and filter out tokens not in the vocabulary\n",
    "\n",
    "- The first step is to **load the vocabulary**. We will use it to filter out words from movie reviews that\n",
    "we are not interested in. \n",
    "\n",
    "You should have a local file called `vocab.txt` with one word per line. We can load that file and build a vocabulary\n",
    "as a set for checking the validity of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = '../output/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we need to **load all of the training data** movie reviews. \n",
    "\n",
    "For that we can adapt the `proces_docs()` from the previous section to:\n",
    "\n",
    " - load the documents, \n",
    " - clean them, and \n",
    " - return them as a **list of strings**, with **one document per string**. \n",
    " \n",
    "> We want each document to be a string for easy encoding as a sequence of integers later. \n",
    "\n",
    "Cleaning the document involves:\n",
    "\n",
    "- splitting each review based on white space, \n",
    "- removing punctuation, and then \n",
    "- filtering out all tokens not in the vocabulary. \n",
    "\n",
    "The updated `clean_doc()` function is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the reviews into a single train or test set and define the class labels\n",
    "\n",
    "The updated `process_docs()` can then call the `clean_doc()` for each document in a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can call the `process_docs` function for both the `neg` and `pos` directories and **combine the reviews into a single train or test dataset**. \n",
    "\n",
    "- We also can **define the class labels** for the dataset.\n",
    "\n",
    "The `load_clean_dataset()` function below will load all reviews and prepare class labels for the training or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('../data/txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('../data/txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconde each document as a sequence of integers\n",
    "\n",
    "The next step is to **encode each document as a sequence of integers**. \n",
    "\n",
    "- The **Keras `Embedding` layer** requires integer inputs where each integer maps to a single token that has a specic\n",
    "real-valued vector representation within the embedding. \n",
    "- These vectors are random at the beginning of training, but during training become meaningful to the network. \n",
    "- We can encode the training documents as sequences of integers using the `Tokenizer` class in the Keras API.\n",
    "\n",
    "First, we must construct an instance of the class then train it on all documents in the training\n",
    "dataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops\n",
    "a consistent mapping from words in the vocabulary to unique integers. We could just as easily\n",
    "develop this mapping ourselves using our vocabulary file. The `create_tokenizer()` function\n",
    "below will prepare a `Tokenizer` from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the mapping of words to integers has been prepared, we can use it to encode the\n",
    "reviews in the training dataset. We can do that by calling the `texts_to_sequences()` function\n",
    "on the Tokenizer. We also need to ensure that all documents have the same length. This is a\n",
    "requirement of Keras for efficient computation. We could truncate reviews to the smallest size\n",
    "or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case,\n",
    "we will pad all reviews to the length of the longest review in the training dataset. First, we can\n",
    "find the longest review using the `max()` function on the training dataset and take its length.\n",
    "We can then call the Keras function `pad_sequences()` to pad the sequences to the maximum\n",
    "length by adding 0 values on the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the maximum length as a parameter to a function to integer encode and\n",
    "pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network model\n",
    "\n",
    "<center><img src=\"../imgs/1D-basics.png\" alt=\"Drawing\" width = \"800\"/></center>\n",
    "<center><p></center>\n",
    "<center><img src=\"../imgs/cnn-schema.png\" alt=\"Drawing\" width = \"800\"/></center>\n",
    "\n",
    "We are now ready to define our neural network model. \n",
    "\n",
    "- The model will use an **`Embedding` layer** as the **first hidden layer**. \n",
    "- The `Embedding` layer requires the specification of the \n",
    "        - vocabulary size, \n",
    "        - the size of the real-valued vector space, and \n",
    "        - the maximum length of input documents. \n",
    "\n",
    "The **vocabulary size** is the total number of words in our vocabulary, plus one for unknown words.\n",
    "This could be the vocab set length or the size of the vocab within the tokenizer used to integer\n",
    "encode the documents, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ##### We will use a **100-dimensional vector space**, but you could try other values, such as 50 or 150. \n",
    "- Finally, the maximum document length was calculated above in the `max_length` variable used during padding. \n",
    "\n",
    "The complete model definition is listed below including the `Embedding`\n",
    "layer. We use a **Convolutional Neural Network (CNN)** as they have proven to be successful\n",
    "at document classification problems. \n",
    "\n",
    "- A conservative CNN configuration is used with **32 filters** (parallel fields for processing words) and \n",
    "- a **kernel size of 8** \n",
    "- with a rectified linear **(`relu`) activation function**. \n",
    "- This is followed by a **pooling layer** that reduces the output of the convolutional layer by half.\n",
    "- Next, the 2D output from the CNN part of the model is attened to one long 2D vector to\n",
    "represent the *features* extracted by the CNN. \n",
    "\n",
    "The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. The output layer uses a sigmoid activation function to output a value between 0 and 1 for the negative and positive sentiment in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='../output/model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running just this piece provides a summary of the defined network. We can see that the\n",
    "Embedding layer expects documents with a length of 1,317 words as input and encodes each\n",
    "word in the document as a 100 element vector.\n",
    "\n",
    "```\n",
    "...\n",
    "_________________________________________________________________\n",
    "Layer (type) Output Shape Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding) (None, 1317, 100) 2576800\n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D) (None, 1310, 32) 25632\n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 655, 32) 0\n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten) (None, 20960) 0\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense) (None, 10) 209610\n",
    "_________________________________________________________________\n",
    "dense_2 (Dense) (None, 1) 11\n",
    "=================================================================\n",
    "Total params: 2,812,053\n",
    "Trainable params: 2,812,053\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "A plot the defined model is then saved to file with the name `model.png`.\n",
    "\n",
    "<center><img src=\"../output/model.png\" alt=\"Drawing\" width = \"450\"/></center>\n",
    "\n",
    "Next, we fit the network on the training data. We use a binary cross entropy loss function\n",
    "because the problem we are learning is a binary classification problem. The efficient Adam\n",
    "implementation of stochastic gradient descent is used and we keep track of accuracy in addition\n",
    "to loss during training. The model is trained for 10 epochs, or 10 passes through the training\n",
    "data. The network configuration and training schedule were found with a little trial and error,\n",
    "but are by no means optimal for this problem. If you can get better results with a different\n",
    "configuration, let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is fit, it is saved to a file named `model.h5` for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('../output/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together. The complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 342\n",
      "Maximum length: 402\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 402, 100)          34200     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 395, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                63050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 122,893\n",
      "Trainable params: 122,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.6935 - acc: 0.5100\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6685 - acc: 0.6022\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.5922 - acc: 0.7383\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.5349 - acc: 0.7878\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.4866 - acc: 0.8261\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.3788 - acc: 0.8717\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.2605 - acc: 0.9189\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.1713 - acc: 0.9544\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.0974 - acc: 0.9828\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0589 - acc: 0.9906\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('../data/txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('../data/txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='../output/model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = '../output/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# save the model\n",
    "model.save('../output/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example will first provide a summary of the training dataset vocabulary (25,768)\n",
    "and maximum input sequence length in words (1,317). The example should run in a few minutes\n",
    "and the fit model will be saved to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model and Make Predictions on New Data\n",
    "\n",
    "In this section, we will look at 2 things:\n",
    "\n",
    "1. Evaluate the model on test set\n",
    "2. Make prediction on new data\n",
    "\n",
    "In this section, we will **evaluate the trained model** and use it to **make predictions on new data**.\n",
    "\n",
    "First, we can use the built-in `evaluate()` function to estimate the skill of the model on both\n",
    "the **training** and test **dataset**. This requires that we load and encode both the training and test\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the model and evaluate it on both datasets and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New data must then be prepared using the same text encoding and encoding schemes as was\n",
    "used on the training dataset. Once prepared, a prediction can be made by calling the `predict()`\n",
    "function on the model. The function below named `predict_sentiment()` will encode and pad\n",
    "a given movie review text and return a prediction in terms of both the percentage and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out this model with two ad hoc movie reviews. The complete example is listed\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 342\n",
      "Maximum length: 402\n",
      "Train Accuracy: 99.33\n",
      "Test Accuracy: 68.00\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: NEGATIVE (51.151%)\n",
      "Review: [This is a good movie. Watch it. I love it.]\n",
      "Sentiment: NEGATIVE (50.816%)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('../data/txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('../data/txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = '../output/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "# load the model\n",
    "model = load_model('../output/model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))\n",
    "\n",
    "# test positive text\n",
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a good movie. Watch it. I love it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename_score = '../data/txt_score/pos/cv_025.txt'\n",
    "text_score = load_doc(filename_score)\n",
    "# test positive text\n",
    "text = text_score\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on new data\n",
    "\n",
    "In this section, we will use the trained model to **make predictions on new data**.\n",
    "\n",
    "The new data comes from [Rotten Tomatoes](https://www.rottentomatoes.com/) movie review aggretator. \n",
    "\n",
    "> Rotten Tomatoes is an American review-aggregation website for film and television. The company was launched in August 1998 by three undergraduate students at the University of California, Berkeley: Senh Duong, Patrick Y. Lee and Stephen Wang.The name \"Rotten Tomatoes\" derives from the practice of audiences throwing rotten tomatoes when disapproving of a poor stage performance. (from [Wikipedia](https://en.wikipedia.org/wiki/Rotten_Tomatoes))\n",
    "\n",
    "The `score.xlsx` located in `../notes/` contains:\n",
    "\n",
    "- 3 Rotten Tomatoes Tomatometer-approved critics\n",
    "- 10 positive and 10 negative reviews per critic (60 reviews)\n",
    "- 85% accurate prediction\n",
    "- We selected the last 10 positive and last 10 negative movie reviews from each critic. The great majority about movies realesed in 2018 and some few in 2017\n",
    "\n",
    "| Critic                 | Positive Reviews  | Negative Reviews  | Total Reviews  |\n",
    "| -------------:         | -------------:| -------------:| ---------:|\n",
    "| Sara Michelle Fetters  | 10         |           10 |      20  |\n",
    "| Jeffrey M. Anderson    | 10         |           10 |      20  |\n",
    "| Pete Hammond           | 10         |           10 |      20  |\n",
    "\n",
    "### Score positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def score_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load doc\n",
    "        text_score = load_doc(path)\n",
    "        # test positive text\n",
    "        percent, sentiment = predict_sentiment(text_score, vocab, tokenizer, max_length, model)\n",
    "        print('Actual Sentiment: POSITIVE Predicted Sentiment: %s (%.3f%%)' % (sentiment, percent*100))\n",
    "\n",
    "# add all docs to vocab\n",
    "score_docs('../data/txt_score/pos', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def score_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load doc\n",
    "        text_score = load_doc(path)\n",
    "        # test positive text\n",
    "        percent, sentiment = predict_sentiment(text_score, vocab, tokenizer, max_length, model)\n",
    "        print('Actual Sentiment: NEGATIVE Predicted Sentiment: %s (%.3f%%)' % (sentiment, percent*100))\n",
    "\n",
    "# add all docs to vocab\n",
    "score_docs('../data/txt_score/neg', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
