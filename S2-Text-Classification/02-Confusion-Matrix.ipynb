{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We mentioned a way to view various metrics of classification is the **confusion matrix**.\n",
    "- Let's explore the basics of the confusion matrix to help us understand precision and recall better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a classification problem, during the testing pahase you will have Two Categories:\n",
    "    - True Condition\n",
    "        - A text message is SPAM\n",
    "    - Predicted Condition\n",
    "        - ML Model predicted SPAM\\*\n",
    "        \n",
    "> \\*Keep in mind it could have also predicted it that it incorrectly as HAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means if you have two possible classes you should have 4 separate groups at the end of testing:\n",
    "\n",
    "- Correctly classified to Class 1: TRUE HAM\n",
    "\n",
    "> So that means you had a message that was HAM and you positively identified it as HAM.\n",
    "\n",
    "- Correctly classified to Class 2: TRUE SPAM\n",
    "\n",
    "> So something was truly SPAM and then you correctly identified it as SPAM.\n",
    "\n",
    "- **Incorrectly** classified to Class 1: FALSE HAM\n",
    "- **Incorrectly** classified to Class 2: FALSE SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to map these out in a little grid we'd have something that looks like this:\n",
    "\n",
    "![](../imgs/i09.png)\n",
    "\n",
    "This is the confusion matrix. If you look it up on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix), which is actually a really helpful article on this, it would map out something that looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to actually look at it a little more simplified for our particular example about text messages:\n",
    "\n",
    "![](../imgs/i10.png)\n",
    "\n",
    "- Again we have the real condition and the predicted condition.\n",
    "- so we can see over on the left hand side if we're going to have two real conditions:\n",
    "    - The real condition is either HAM \n",
    "    - or real condition SPAM\n",
    "- And then along the columns we have our predicted condition:\n",
    "    - Predicting HAM or \n",
    "    - Predicting SPAM\n",
    "\n",
    "- You'll notice that if the real condition is HAM and we predicted HAM then we have a **true positive**.\n",
    "- Along the predicted condition, We have a **false negative**. That means that the real condition was HAM but our machine learning model incorrectly predicted it to be SPAM.\n",
    "\n",
    "> Here we are relabeling HAM as positive and SPAM as negative.\n",
    "\n",
    "- We also see that we have real conditions SPAM and then predicted HAM.That's known as a **false positive** that falsely identifying something to the positive class which in this case is HAM.\n",
    "- We also finally **true negative** correctly identifying something to the negative class predicting SPAM for SPAM text message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we come back to that other confusion matrix that we saw earlier, we can expand on this to have quite a wide variety of different metrics we can calculate. Things like **true positive rate**, **false positive rate**, **positive likelihood ratio**, **false emission rate**, etc. But really we're just concerned for a few of these. \n",
    "\n",
    "We're concerned with **Recall**, **Accuracy** and **Precision**.\n",
    "\n",
    "![](../imgs/i11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main point to remember is the confusion matrix and the various calculated metrics is that they are all fundamentally ways of comparing the predicted values versus the true values.\n",
    "- What constitutes \"good\" metrics will really depend on the specific situation. In some situations 99% accuracy is fantastic. In other situations 99% accuracy may actually not be good enough for whatever you're are trying to predict because maybe it comes at the cost of a really poor precision and poor recall.\n",
    "\n",
    ">So we can't just say that there's certain good values for particular metrics. Obviously if you get 100% across precision, accuracy and recall then you have a really good model. But in the real world you're probably not going to get 100% of all those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "A **confusion matrix** is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are already known.\n",
    "\n",
    "The confusion matrix itself is actually relatively simple to understand but sometimes the related terminology can be confusing.\n",
    "\n",
    "![](../imgs/logistic6.png)\n",
    "\n",
    "\n",
    "\n",
    "In this case we have a binary classification problem. So in this example we're testing for the presence of a disease where no is a negative test which is false equals zero. Yes ia positive test when True is equal to 1. \n",
    "\n",
    "There are two possible predictive classes Yes, and NO, \n",
    "\n",
    "If we were predicting the presence of a disease for example yes it would mean that they have disease.\n",
    "\n",
    "The classifier made a total of a 165 predictions meaning 165 patients were tested for the presence of the disease.\n",
    "\n",
    "Out of those 165 cases the classifier predicted a yes 110 times and no 55 times.\n",
    "\n",
    "In reality meaning we already have label test data 105 patients in the sample have the disease and 60 patients do not.\n",
    "\n",
    "Now let's go ahead and define the most basic terms the basic terms are the whole number it terms so not rates just hold numbers and those terms are true positives true negatives false positives and false negatives.\n",
    "\n",
    "![](../imgs/logistic7.png)\n",
    "\n",
    "\n",
    "Then we can talk about rates. The first rate we can discuss is **accuracy**.\n",
    "\n",
    "![](../imgs/logistic8.png)\n",
    "\n",
    "What this is actually getting at is overall how often is it correct.\n",
    "\n",
    "A lot of times when you hear reports on studies they'll just tell you the accuracy and the accuracy is calculated by the number of true positives plus the number of true negatives over the total.\n",
    "\n",
    "\n",
    "\n",
    "Then we have the misclassification rate which is answering the question. Overall how often is the model wrong.\n",
    "\n",
    "![](../imgs/logistic9.png)\n",
    "\n",
    "This is going to be calculated by the number of false positives plus a number of false negatives divided\n",
    "\n",
    "by the total.\n",
    "\n",
    "So that's 15 divided by a five.\n",
    "\n",
    "Overall this is 9 percent error rate or misclassification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
